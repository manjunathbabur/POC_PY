import time
# --->  REMOVE PYSPARK REFERENCES

# import pyspark.sql.functions as f
# from pyspark.sql import SparkSession
# from pyspark.sql.functions import udf,col
# from pyspark.sql.types import IntegerType
# spark = SparkSession.builder.appName("DataEngeering1").getOrCreate()


# <---  REPLACE WITH SNOWPARK REFERENCES (Rest of code is almost identical)

import snowflake.snowpark.functions as f
from snowflake.snowpark import Session, DataFrame
from snowflake.snowpark.functions import udf, col
from snowflake.snowpark.types import IntegerType
from snowflake.snowpark.functions import call_udf


# <----- Make these changes before running the notebook -------
# Change Connection params to match your environment
# <----------------------------------------------------------------------------

Warehouse_Name = 'MY_DEMO_WH'
Warehouse_Size = "LARGE"
DB_name = 'DEMO_SNOWPARK'
Schema_Name = 'Public'


CONNECTION_PARAMETERS= {
    'account': '<Snowflake_Account_Locator>',
    'user': 'SomeUser',
    'password': 'Not4u2Know',
    'role': 'SYSADMIN'
}

print("Connecting to Snowflake.....\n")
session = Session.builder.configs(CONNECTION_PARAMETERS).create()
print("Connected Successfully!...\n")

sql_cmd = f"CREATE OR REPLACE WAREHOUSE {Warehouse_Name} WAREHOUSE_SIZE = 'X-Small'  AUTO_SUSPEND = 10 "
print("XS Cluster Created & Ready \n")

session.sql(sql_cmd).collect() 

sql_cmd = f"CREATE OR REPLACE DATABASE {DB_name}"
session.sql(sql_cmd).collect() 
print("Database is Created & Ready \n")

session.use_database(DB_name)
session.use_schema(Schema_Name)
session.use_warehouse(Warehouse_Name)
